---
title: 'Building AI Applications with MCP: A Complete Guide Using LangChain and Gemini'
description: 'Learn to build intelligent AI applications using Model Context Protocol (MCP) with LangChain, langchain-mcp-adapter, and Google Gemini models. From setup to deployment, create a fully functional CLI chat application.'
pubDate: 'Sep 15 2024'
group: "AI"
githubHref: "https://github.com/Infernus007/jash-naik-blogs/tree/master/blogs-examples/mcp-cli-chat"
tags: ["MCP", "LangChain", "Gemini", "AI", "CLI", "Python", "Chat", "Tools"]
---

import Placeholder from "../../components/blog/CodeBlock/index.astro"
import ShowCase from "../../components/blog/ShowCase/index.astro"
import LinkPreview from "../../components/LinkPreview/index.astro"

Imagine building an AI assistant that can actually read your documents, edit files, and remember context between conversations. Not just another chatbot, but an AI that has real capabilities. Today, we're building exactly that using the Model Context Protocol (MCP), LangChain, and Google's Gemini models.

<ShowCase 
  content={[
    "Interactive CLI chat application",
    "AI that reads and manages documents",
    "Auto-completion and intelligent suggestions",
    "Real-time streaming responses",
    "Complete MCP implementation from scratch"
  ]} 
  type="info"
/>

## What We're Building

We're creating an intelligent CLI chat application where you can have conversations like this:

- "What's in the financial report?"
- "Summarize all the meeting notes"
- "Edit the project plan to include the new deadline"

Our AI will automatically find the right documents, read them, and give you intelligent responses based on your actual data. Along the way, you'll learn MCP fundamentals: **Tools** (functions AI can execute), **Resources** (data AI can access), and **Prompts** (templates for consistent behavior).

*[Placeholder for application demo screenshot]*

## Prerequisites

Before we start, make sure you have:

- **Python 3.10+** installed
- **uv** package manager ([install here](https://docs.astral.sh/uv/))
- **Google API key** for Gemini ([get one here](https://makersuite.google.com/app/apikey))
- **Node.js 18+** for the MCP Inspector tool

We'll use `uv` throughout this tutorial as it provides fast, reliable package management and makes our MCP setup much smoother.

## Creating the Core Features

Let's start by building the heart of our application - the chat system that will eventually connect to our MCP server. We'll begin with a simple version and progressively add more capabilities.

First, let's set up our project:

<Placeholder>
```bash showLineNumbers caption="Project Setup" title="terminal"
# Create project directory
mkdir mcp-chat-app && cd mcp-chat-app

# Initialize with uv
uv init .

# Add our dependencies
uv add langchain-mcp-adapters langgraph langchain-google-genai mcp python-dotenv
```
</Placeholder>

Now let's create our basic chat system:

<Placeholder>
```python showLineNumbers caption="Basic Chat System" title="core/chat.py"
import os
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.prebuilt import create_react_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_mcp_adapters.tools import load_mcp_tools

class Chat:
    def __init__(self, clients: dict = None):
        self.clients = clients or {}
        self.messages: list[BaseMessage] = []
        self.agent = None

    async def initialize_agent(self):
        # Start with no tools - we'll add MCP tools later
        tools = []
        
        # Load MCP tools if we have clients
        for client in self.clients.values():
            tools.extend(await load_mcp_tools(client.session()))
        
        # Initialize Gemini
        gemini_model = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")
        llm = ChatGoogleGenerativeAI(model=gemini_model)
        
        # Create ReAct agent
        self.agent = create_react_agent(llm, tools)

    async def run(self, query: str):
        if not self.agent:
            await self.initialize_agent()

        self.messages.append(HumanMessage(content=query))
        
        async for event in self.agent.astream_events(
            {"messages": self.messages}, version="v1"
        ):
            yield event
```
</Placeholder>

## Simple CLI Interface

Let's create a basic CLI that we can test our chat system with:

<Placeholder>
```python showLineNumbers caption="Simple CLI" title="main.py"
import asyncio
import os
from dotenv import load_dotenv
from core.chat import Chat

load_dotenv()

async def main():
    chat = Chat()
    
    print("ðŸ¤– Basic Chat Ready! (No MCP tools yet)")
    print("Try: 'Hello, how are you?'")
    print("Type 'quit' to exit\n")
    
    while True:
        try:
            user_input = input("> ").strip()
            
            if user_input.lower() in ['quit', 'exit']:
                break
                
            if not user_input:
                continue
                
            print("\nAI: ", end="", flush=True)
            async for event in chat.run(user_input):
                if event.get("event") == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk")
                    if chunk and hasattr(chunk, 'content'):
                        print(chunk.content, end='', flush=True)
            print("\n")  # New line after response
                        
        except KeyboardInterrupt:
            break
    
    print("ðŸ‘‹ Goodbye!")

if __name__ == "__main__":
    asyncio.run(main())
```
</Placeholder>

Create a `.env` file with your Google API key:

<Placeholder>
```bash showLineNumbers caption="Environment Variables" title=".env"
GOOGLE_API_KEY=your_google_api_key_here
GEMINI_MODEL=gemini-2.0-flash-exp
```
</Placeholder>

Now you can test the basic chat:

<Placeholder>
```bash showLineNumbers caption="Testing Basic Chat" title="terminal"
uv run main.py
```
</Placeholder>

*[Placeholder for basic chat screenshot]*

Great! We have a working chat, but it's just a regular AI - no special capabilities yet. Let's add MCP to give our AI some superpowers.

## Building the MCP Server

Now we'll create an MCP server that manages documents and provides tools for our AI to use. This is where the magic happens:

<Placeholder>
```python showLineNumbers caption="MCP Document Server" title="mcp_server.py"
from mcp.server.fastmcp import FastMCP
from pydantic import Field
from mcp.server.fastmcp.prompts import base

# Initialize MCP server
mcp = FastMCP("DocumentMCP", log_level="ERROR")

# Simple document storage (in production, use a database)
docs = {
    "report.pdf": "The report details the state of a 20m condenser tower. Current efficiency is at 87% with potential improvements identified.",
    "financials.docx": "Q3 financials show revenue growth of 23% compared to last quarter. Operating expenses increased by 12%.",
    "plan.md": "Project implementation plan: Phase 1 (Q1): Infrastructure setup. Phase 2 (Q2): Core development. Phase 3 (Q3): Testing and deployment.",
    "notes.txt": "Meeting notes from Nov 15: Discussed budget allocation, timeline adjustments, and resource requirements for upcoming phases."
}

@mcp.tool(
    name="read_doc_contents",
    description="Read the contents of a document and return it as a string."
)
def read_document(doc_id: str = Field(description="Id of the document to read")):
    """Read a document by its ID"""
    if doc_id not in docs:
        raise ValueError(f"Document {doc_id} not found")
    return docs[doc_id]

@mcp.tool(
    name="edit_document", 
    description="Edit a document by replacing text"
)
def edit_document(
    doc_id: str = Field(description="Id of the document to edit"),
    old_text: str = Field(description="Text to replace"),
    new_text: str = Field(description="New text to insert")
):
    """Edit a document by replacing old text with new text"""
    if doc_id not in docs:
        raise ValueError(f"Document {doc_id} not found")
    
    if old_text not in docs[doc_id]:
        raise ValueError(f"Text '{old_text}' not found in document {doc_id}")
    
    docs[doc_id] = docs[doc_id].replace(old_text, new_text)
    return f"Successfully updated {doc_id}"

if __name__ == "__main__":
    mcp.run(transport="stdio")
```
</Placeholder>

## Creating the MCP Client

Now we need a client to connect to our MCP server:

<Placeholder>
```python showLineNumbers caption="MCP Client" title="mcp_client.py"
import sys
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client
from typing import Optional, Any
import json

class MCPClient:
    def __init__(self, command: str, args: list[str], env: Optional[dict] = None):
        self._command = command
        self._args = args
        self._env = env
        self._session: Optional[ClientSession] = None
        self._exit_stack: AsyncExitStack = AsyncExitStack()

    async def connect(self):
        server_params = StdioServerParameters(
            command=self._command,
            args=self._args,
            env=self._env,
        )
        
        stdio_transport = await self._exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        
        _stdio, _write = stdio_transport
        self._session = await self._exit_stack.enter_async_context(
            ClientSession(_stdio, _write)
        )
        await self._session.initialize()

    def session(self) -> ClientSession:
        if self._session is None:
            raise ConnectionError("Client not connected")
        return self._session

    async def list_tools(self) -> list[types.Tool]:
        result = await self.session().list_tools()
        return result.tools

    async def __aenter__(self):
        await self.connect()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self._exit_stack.aclose()
```
</Placeholder>

Let's update our main application to use the MCP client:

<Placeholder>
```python showLineNumbers caption="Updated Main Application" title="main.py"
import asyncio
import os
from dotenv import load_dotenv
from contextlib import AsyncExitStack

from core.chat import Chat
from mcp_client import MCPClient

load_dotenv()

async def main():
    command = "uv"
    args = ["run", "mcp_server.py"]
    
    async with AsyncExitStack() as stack:
        # Connect to MCP server
        doc_client = await stack.enter_async_context(
            MCPClient(command=command, args=args)
        )
        
        # Initialize chat with MCP client
        chat = Chat(clients={"doc_client": doc_client})
        
        print("ðŸ¤– MCP Chat Ready!")
        print("Try: 'What documents do you have access to?'")
        print("Or: 'Read the financial report'")
        print("Type 'quit' to exit\n")
        
        while True:
            try:
                user_input = input("> ").strip()
                
                if user_input.lower() in ['quit', 'exit']:
                    break
                    
                if not user_input:
                    continue
                    
                print("\nAI: ", end="", flush=True)
                async for event in chat.run(user_input):
                    if event.get("event") == "on_chat_model_stream":
                        chunk = event.get("data", {}).get("chunk")
                        if chunk and hasattr(chunk, 'content'):
                            print(chunk.content, end='', flush=True)
                print("\n")
                            
            except KeyboardInterrupt:
                break
        
        print("ðŸ‘‹ Goodbye!")

if __name__ == "__main__":
    asyncio.run(main())
```
</Placeholder>

Now test it out:

<Placeholder>
```bash showLineNumbers caption="Testing with MCP Tools" title="terminal"
uv run main.py
```
</Placeholder>

*[Placeholder for MCP tools in action screenshot]*

Amazing! Your AI can now read and edit documents. But we can make it even better by adding Resources for data access and Prompts for consistent behavior.

## Enriching with Resources

Resources provide read-only access to data. Let's add them to our MCP server:

<Placeholder>
```python showLineNumbers caption="Adding Resources to MCP Server" title="mcp_server.py" 
# Add these to your existing mcp_server.py

@mcp.resource("docs://documents", mime_type="application/json")
def list_docs() -> list[str]:
    """List all available documents"""
    return list(docs.keys())

@mcp.resource("docs://documents/{doc_id}", mime_type="text/plain")
def fetch_doc(doc_id: str) -> str:
    """Fetch a specific document by ID"""
    if doc_id not in docs:
        raise ValueError(f"Document {doc_id} not found")
    return docs[doc_id]
```
</Placeholder>

## Adding Prompts for Consistency

Prompts help maintain consistent AI behavior. Let's add some useful ones:

<Placeholder>
```python showLineNumbers caption="Adding Prompts to MCP Server" title="mcp_server.py"
# Add these to your existing mcp_server.py

@mcp.prompt(
    name="summarize",
    description="Summarize document contents concisely"
)
def summarize_document(
    doc_id: str = Field(description="Id of document to summarize")
) -> list[base.Message]:
    """Generate a summary prompt for a document"""
    prompt = f"""
    Please provide a concise summary of the document '{doc_id}'.
    Focus on the key points and main takeaways.
    Keep it brief but informative.
    """
    return [base.UserMessage(prompt)]

@mcp.prompt(
    name="format",
    description="Format document into clean markdown"
)
def format_document(
    doc_id: str = Field(description="Id of document to format")
) -> list[base.Message]:
    """Generate a formatting prompt for a document"""
    prompt = f"""
    Please format the document '{doc_id}' into clean, well-structured markdown.
    Organize the content with appropriate headers, bullet points, and formatting.
    Only return the formatted content, no explanations.
    """
    return [base.UserMessage(prompt)]
```
</Placeholder>

## Testing with MCP Inspector

The MCP Inspector is a fantastic tool for debugging and exploring your MCP server. Let's use it to see what we've built:

<Placeholder>
```bash showLineNumbers caption="Using MCP Inspector" title="terminal"
# Install and run the MCP Inspector
npx @modelcontextprotocol/inspector uv run mcp_server.py
```
</Placeholder>

This opens a web interface where you can:
- See all your tools, resources, and prompts
- Test tool calls interactively
- Explore resource URIs
- Debug your MCP implementation

*[Placeholder for MCP Inspector interface screenshot]*

*[Placeholder for resource auto-completion screenshot]*

*[Placeholder for tool call demonstration screenshot]*

The Inspector shows you exactly what your AI can see and do - it's invaluable for development and debugging.

## Your Complete MCP Application

Congratulations! You've built a complete MCP-powered AI application. Here's what you've accomplished:

âœ… **Core chat system** with LangChain and Gemini integration  
âœ… **MCP server** with tools for document management  
âœ… **MCP client** for seamless communication  
âœ… **Resources** for efficient data access  
âœ… **Prompts** for consistent AI behavior  
âœ… **Inspector integration** for development and debugging  

Your AI can now:
- Read and edit documents intelligently
- Access data through structured resources
- Use consistent prompts for better responses
- Handle complex multi-step operations
- Provide real-time streaming responses

*[Placeholder for complete application demo screenshot]*

Test your application with queries like:
- "What's the current efficiency in the report?"
- "Summarize all the documents"
- "Edit the plan to extend Phase 2 by one month"
- "Format the meeting notes as markdown"

<div className="my-8">
  <LinkPreview 
    url="https://github.com/Infernus007/jash-naik-blogs/tree/master/blogs-examples/mcp-cli-chat"
    width={400}
    height={200}
  >
    ðŸš€ Complete Implementation with Advanced Features
  </LinkPreview>
</div>

The full example includes additional features like command auto-completion, CLI interface. You now have everything you need to build sophisticated AI applications that can actually interact with real-world data and systems.
